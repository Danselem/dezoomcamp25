id: 08_gcp_liquor
namespace: zoomcamp
description: |
  Iowa Liquor Sales Data from Google Bigquery

inputs:
- id: liquor
  type: SELECT
  displayName: Select taxi type
  values: [ iowa, chicago ]
  defaults: iowa
- id: year
  type: SELECT
  displayName: Select year
  values: [ "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "2024" ]
  defaults: "2012"
  allowCustomValue: true # allows you to type 2021 from the UI for the homework ðŸ¤—

- id: month
  type: SELECT
  displayName: Select month
  values: [ "01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12" ]
  defaults: "01"

variables:
  file: "{{inputs.liquor}}_sales_{{inputs.year}}-{{inputs.month}}.csv"
  gcs_bucket: "gs://{{kv('GCP_BUCKET_NAME')}}"
  aws_bucket: "{{kv('AWS_BUCKET_NAME')}}"
  aws_location: "{{kv('AWS_LOCATION')}}"
  gcs_file: "{{vars.gcs_bucket}}/{{vars.file}}"
  aws_file: "iowa_sales/{{vars.file}}"
  table: "{{kv('GCP_DATASET')}}.{{inputs.liquor}}_sales_{{inputs.year}}_{{inputs.month}}"
  data: "{{outputs.extract.outputFiles[inputs.liquor ~ '_sales_' ~ inputs.year ~ '-' ~ inputs.month ~ '.csv']}}"

tasks:
- id: set_label
  type: io.kestra.plugin.core.execution.Labels
  labels:
    file: "{{render(vars.file)}}"
    liquor: "{{inputs.liquor}}"

- id: extract
  type: io.kestra.plugin.scripts.python.Script
  outputFiles:
  - "*.csv"
  description: Merge parquet files using Python
  containerImage: ghcr.io/kestra-io/pydata:latest # Use Python container with pandas installed
  beforeCommands:
  - pip install pandas requests google-cloud-bigquery google-cloud-bigquery-storage db-dtypes
  - pip install --upgrade google-cloud-storage

  script: |
    import os
    import ast
    from google.cloud import bigquery
    import pandas
    import pandas as pd
    from google.oauth2 import service_account

    credentials = service_account.Credentials.from_service_account_info({{ kv('GCP_CREDS') }})

    project_id = "{{kv('GCP_PROJECT_ID')}}" # _set_env('PROJECT_ID')

    client = bigquery.Client(location="US",credentials=credentials, project=project_id)
    query = f"""
      SELECT *,
      SAFE_CAST(REGEXP_REPLACE(zip_code, r'\.0$', '') AS INT64) AS zip_code_int,
      ST_X(store_location) AS longitude,
      ST_Y(store_location) AS latitude
      FROM bigquery-public-data.iowa_liquor_sales.sales
      WHERE store_location IS NOT NULL AND 
      EXTRACT(YEAR FROM date)={{inputs.year}}
      AND EXTRACT(MONTH FROM date)={{inputs.month}}"""
    # print(query)
    query_job = client.query(query)

    df = query_job.to_dataframe() # Wait for the job to complete.
    df.drop(columns=['store_location'], inplace=True)
    df.date = pd.to_datetime(df.date)
    # df.updated_on = pd.to_datetime(df.updated_on)

    # Save as one csv
    df.to_csv("{{render(vars.file)}}", index=False)

- id: upload-to-gcs
  type: io.kestra.plugin.gcp.gcs.Upload
  description: Upload the merged Parquet file to GCS
  from: "{{render(vars.data)}}"
  to: "{{render(vars.gcs_file)}}"

- id: aws_s3_upload
  type: io.kestra.plugin.aws.s3.Upload
  accessKeyId: "{{ kv('AWS_ACCESS_KEY_ID') }}"
  secretKeyId: "{{ kv('AWS_SECRET_ACCESS_KEY') }}"
  region: "{{render(vars.aws_location)}}"
  from: "{{render(vars.data)}}"
  bucket: "{{render(vars.aws_bucket)}}"
  key: "{{render(vars.aws_file)}}"

- id: purge_files
  type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
  description: To avoid cluttering your storage, we will remove the downloaded files
  disabled: true

pluginDefaults:
- type: io.kestra.plugin.gcp
  values:
    serviceAccount: "{{kv('GCP_CREDS')}}"
    projectId: "{{kv('GCP_PROJECT_ID')}}"
    location: "{{kv('GCP_LOCATION')}}"
    bucket: "{{kv('GCP_BUCKET_NAME')}}"
